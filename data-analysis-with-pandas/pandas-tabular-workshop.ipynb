{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy and explore tabular data with pandas\n",
    "#### A *Python for Reproducible Research* workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this lesson, you'll get a crash course in bringing your tabular research data into Python. \n",
    "\n",
    "We will focuses on **pandas**, a key part of the scientific Python ecosystem. We'll also introduce you to **NumPy**, the powerful framework underlying pandas and **matplotlib**, a visualization library already integrated into pandas. (We'll use the **seaborn** visualization library briefly too). Everything we do today will happen in this **Jupyter Notebook** - a tool for composing and sharing computational narratives that is becoming increasingly popular across the sciences.\n",
    "\n",
    "At the end, you'll have written a data analysis recipe you can apply to future datasets and research questions. One way to think of this kind of recipe is a **data science pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://d33wubrfki0l68.cloudfront.net/795c039ba2520455d833b4034befc8cf360a70ba/558a5/diagrams/data-science-explore.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A pipeline for exploratory data analysis (Grolemund and Wickham, 2017)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the pipeline stages we'll focus on today:\n",
    "\n",
    "* **Stage 0:** Identify research questions & related data\n",
    "* **Stage 1:** Import your data into a pandas dataframe\n",
    "* **Stage 2:** Tidy your data: identify and respond to weirdness!\n",
    "* **Stage 3:** Explore data: summarize, visualize, plot\n",
    "\n",
    "And we'll show you what you'll need for one additional step:\n",
    "\n",
    "* **Stage 4:** Evaluate modeling methods (e.g. classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Identify research questions & related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study: Breast Cancer Dianostic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://upload.wikimedia.org/wikipedia/commons/8/8b/Breast_fibroadenoma_by_fine_needle_aspiration_%282%29_PAP_stain.jpg', width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fine needle aspiration of fibroadenoma, a type of benign breast tumor. (Source: Wikimedia)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll be using the **Wisconsin Diagnostic Breast Cancer (WDBC) dataset**. \n",
    "\n",
    "The [original repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\\(Diagnostic\\)) contains several versions of the breast cancer data. For today we're interested in Dr. Wolberg's original encodings of the FNA image data, which uses a ranking of 1 to 10 to describe a variety of attributes such unifority of cell shape and size, mitoses and normal nuclei count, margin adhesion, clump thickness, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we are researchers at Big University Oncology Institute working with fine needle aspirate (FNA) images of breast masses. Our long-term goal is to increase the efficacy of clinicians in distinguishing malignant from non-malignant masses in FNA digital images.\n",
    "\n",
    "Given our research questions, how might we wish to explore a dataset where observations correspond to fine needle aspiration (FNA) images of breast growths? What's important to us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Edit this box to include a question you would like to investigate in this data.**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import your data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First, let's import the pandas and numpy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and numpy; assign aliases pd and np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_data_URL = 'https://raw.githubusercontent.com/zoews/tidy-tuesday/master/wisconsin_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV into a dataframe using the appropriate pandas method\n",
    "# assign this new dataframe to the variable diagnostic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, a DataFrame is a data type that allows us to use special functions. When a function is relative to an object, we call it a **method** and access it with a period at the end of the object. So, for example, print is a standalone function:\n",
    "\n",
    "```\n",
    "print(\"Hello world\")\n",
    "```\n",
    "\n",
    "But .head() is a pandas **method** that must be called on a pandas DataFrame:\n",
    "\n",
    "```\n",
    "diagnostic_data.head()\n",
    "```\n",
    "\n",
    ".head() usefully shows us the first several rows in our DataFrame along with row and column labels. Let's try it below, passing in `20` to show the first 20 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! Take a look at the header line. What do you think is going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv data into diagnostic_data again, but this time specify no header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, halfway there! We can use the DataFrame as is, but it will be frustrating not to rely on actual column names. We need to dig into the data documentation itself to figure out the best labels in this case. \n",
    "\n",
    "Here's the relevant part of our data readme:\n",
    "\n",
    "```\n",
    "7. Attribute Information: (class attribute has been moved to last column)\n",
    "\n",
    "   #  Attribute                     Domain\n",
    "   -- -----------------------------------------\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "  10. Mitoses                       1 - 10\n",
    "  11. Class:                        (2 for benign, 4 for malignant)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign a header to a DataFrame without column labels (or to replace existing Column labels), we can use a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_labels = ['sample_code', \n",
    "                    'clump_thickness', \n",
    "                    'uniformity_cell_shape', \n",
    "                    'uniformity_cell_size',\n",
    "                    'marginal_adhesion',\n",
    "                    'single_epi_cell_size',\n",
    "                    'bare_nuclei',\n",
    "                    'bland_chrom',\n",
    "                    'norm_nuclei',\n",
    "                    'mitoses',\n",
    "                    'mass_class'\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the labels to the .columns attribute of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a nicely labeled table output to the Notebook above. Congrats! Your data is definitively imported into a DataFrame.\n",
    "\n",
    "Note that pandas will create an index and assign ordered values beginning at 0 by default. You can override this behavior (as in, you could set sample_code as the index) but it's helpful to have this index in place for future use, e.g. if we wanted to reverse the order of our data and sample_code wasn't sequential. [Here is a helpful resource](https://t.co/swUeOFoc33?amp=1) for learning more about index behavior in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tidy your data: identify and respond to weirdness!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us to consider a dataset a tidy dataset, it must follow three principles (Wickham, 2013):\n",
    "\n",
    "> 1. Each variable forms a column.\n",
    "> 2. Each observation forms a row.\n",
    "> 3. Each type of observational unit forms a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also express this visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Source: [\"12. Tidy Data\"](https://r4ds.had.co.nz/tidy-data.html), *R for Data Science*. Wickham and Grolemund, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already made one significant change to make our DataFrame **tidy** - manually declaring a header! If we leave the distinction between header and first row ambiguous, we lose the integrity of the tidy data structure where each row corresponds to an observation, and *only* an observation.\n",
    "\n",
    "Another concern is empty, missing, or **null values**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hunting for nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for null values in our diagnostic data, we may wish to use the following syntax: `diagnostic_data.isnull().any()`. What this does:\n",
    "\n",
    "* 'diagnostic_data' is our DataFrame of FNA image data observations\n",
    "* `.isnull()` is a method that returns True or False if a null value is found, per value in our dataframe\n",
    "* `.any()` looks for a single True value in any column and returns the outcome.\n",
    "\n",
    "Let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if nulls exists in any observation, by variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, right? However, we are assuming that pandas would spit out nulls in the DataFrame - what if our DataFrame is not operating under these principles?\n",
    "\n",
    "Let's take a look using the `.info()` method (or we could use the `.dtypes` attribute, your choice!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data types in your dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice?\n",
    "\n",
    "The `.unique()` method can be used on a DataFrame column to show all unique values. For instance, if we want to see all of the unique values for mitoses, we could run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.mitoses.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code below to view the unique values in our DataFrame showing unexpected behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code below that calls the unique method on the column we are investigating:\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To chck how prevelant the unexpected value is in our data, let's replace the `.unique()` method call with `.value_counts()`, which provides counts for each unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code below that generates counts for each unique value in the column we are investigating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To treat this variable as numeric, but also include null values, we must use the floating point numeric data type. If we try to convert this data directly to floating point right now, however, it will produce an error. We thus need to use a two-step process:\n",
    "\n",
    "* `.replace()` to convert our offending character to NumPy NaN object, which you can generate with `np.nan`\n",
    "* `.astype()` which will allow us to coerce eligible data to floating point numeric, or `'float64'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: replace strange characters with np.nan in our target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: coerce the entire column into 'float64' data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see if our DataFrame now includes nulls that pandas can recognize, and also stores bare-nuclei in the correct data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call .isnull().any() in your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check .info() on the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you must make a decision: do you exclude observations with null values? Why or why not? If you wanted to exclude those observations, you could filter by null or non-null.\n",
    "\n",
    "To set a filter condition, you use brackets with a statement to evaluate in Python. Ex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data[diagnostic_data.clump_thickness > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for a null value, pandas requires us to use the `.isnull()` method which will return either true or false. If we want to negate this, we can use a `!` beforehand, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data[!diagnostic_data.clump_thickness.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you use this filter to exclude nulls where we expect to find them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below -- assign if you wish, evaluate but don't assign if you prefer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode benign/malignant class as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find counts per class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just used the `.replace()` method above to properly note nulls. In addition to replacing a single value, we can replace multiple values at the same time. To do this, we can pass in a dictionary.\n",
    "\n",
    "Uncomment the block below and replace the number-string pairs with the class labels provided for our data (see the '7. Attribute Information' table above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_replace = {#: '???',\n",
    "#              #: '???'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's coerce the entire column to be of type 'category'. Enter the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coerce relevant variable to category type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anything else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: nunique() gives count of unique values (as opposed to total number of values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore your data: summarize, visualize, transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMINDER: What are our research questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify some specific tasks that may be helpful:\n",
    "* Learn the overall size, shape, and scope of our FNA imaging data.\n",
    "* Summarize the distribution for each image attribute.\n",
    "* Explore possible correlations between image attributes.\n",
    "* Aggregate observations by class and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Learn the overall size, shape, and scope of our FNA imaging data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When getting oriented to a new dataset, I like to see a few examples of observations, and also make sure the data is in a format I expect.\n",
    "\n",
    "We've already seen `.head()` and `.info()`. Let's also add in `.tail()`, which just shows the bottom 5 items. We can also pass in a parameter to see the nth rows from the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to quickly view the overall dimensions of your DataFrame, the most concise way is to use `.shape`\n",
    "\n",
    "Unlike .head(), which is a **method** (or function that belongs to our DataFrame object), .shape is an **atribute** meaning that it returns some data about the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you think shape uses .shape or .shape() syntax? Try it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Summary statistics and distributions for each image attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a big-picture view of how our dataframe is organized, but we don't yet know much about the individual columns. What is the distribution of values within a given column? How do they compare to one another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the `.columns` attribute call on our dataframe to remind us of all available columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get a visual sense of the distirbution of values within each column. First, let's see this as a big matrix using the .describe() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe your dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas supports visualization using the matplotlib library natively, which is very helpful for our purposes. To make this work within a notebook, we simply need to import matplotlib, and then specify that we want to see inline visualizations with a \"magic command\" (the magic command is specific to Jupyter Notebooks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate side-by-side histograms of all our columns by calling the `.hist()` method on our DataFrame. It's not necessary to specify paramters, but for readability, it's helpful to set the `figsize` parameter to an `(x, y)` tuple, in inches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_plot = diagnostic_data.hist(figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pair this code with the column select syntax `DataFrame[[\"Column A\", \"Column B\"...]]` to compare just a subset of columns. Unlike the dot format of referencing a column (which is my favorite for most situations), the bracket format allows you to pass in a list of columns, which is helpful for subsetting. If you write code this way, note that you need two brackets, and the column names must now be passed in as strings (and thus must be surrounded by quotation marks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a comparison_plot that only focuses on two columns, in contrast to the our_plot code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add your own code to compare the distribution of two other variables here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore a given column. You can always start with describe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call describe on clump_thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you may wish to extract specific information such as the mean, median, sum, count, etc. All of these can be called as methods on pandas DataFrame column (which is equivalent to a pandas Series), such as `df.MY_COLUMN.sum()` or `df.MY_COLUMN.mean()`. Try it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write this method call using the bracket notation for referencing a column, such as df['MY_COLUMN'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Possible correlations between image attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize correlations!\n",
    "\n",
    "We can start with calling the .corr() method on our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, assign correlation matrix to variable\n",
    "# use plt.matshow() to generate a matrix visualization\n",
    "# then use plt.show() to print it out to the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matplotlib is only so-so at visualization correlations. Let's use seaborn instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call sns.heatmap()\n",
    "# this variable takes three parameters: the correlation matrix\n",
    "# xticklabels, which we can assign to the .columns attribute of our correlation matrix\n",
    "# and yticklabels, which we can ALSO assign to the .columns attribute of our correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to try more seaborn visualizations, take a look at some [more examples](https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Aggregate observations by class and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will focus on the `.groupby()` method and the concept of the **index** in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by our output class benign/malignant and describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by our output class malignant and call hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.hist()` is a good start, but let's use seaborn to see a little more clearly.\n",
    "\n",
    "Let's use [catplot](https://seaborn.pydata.org/tutorial/categorical.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate modeling methods (e.g. classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've tidied up our dataset, including dealing with headers and nulls; glimpsed the design, shape, and typical observations within our datset; explored summary statistics and distributions of values within columns; and evaluated covariance between columns with correlation plots and heat maps. \n",
    "\n",
    "Each of these tasks are valuable in their own right, no matter what analytical method we wish to pursue next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's save our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnostic_data.to_csv(\"wisconsin_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avenues for subsequent analysis (e.g. designing a machine learning classification task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data work you've done in pandas is *particularly* relevant if we plan to approach this research question\n",
    "\n",
    "> we are interested in whether patterns exist in imaging data that would improve the ability of clinicians to correctly dinstinguish malignant and non-malignant tumors\n",
    "\n",
    "as the classification task\n",
    "\n",
    "> Given the output class of \"malignant\" or \"benign\" in our observations, can we generate an accurate and reliable prediction from observations about Fine Needle Aspiration images (also contained within our observations)? If so, what variables are singificant in predicting an outcome class effectively? Among those important predictors, how would a change in measurement value influence the likely outcome, and how confident are we in that influence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in a tidy dataframe, those observations include a outcome class variable as well as several storng candidate for predictor variables. This is an excellent task to pursue via the supervised machine learning method of [binary classification](https://www.sciencedirect.com/topics/computer-science/binary-classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Python ecosystem, most if not all popular machine learning packages will work natively with pandas DataFrames. Look out for a future lesson using random forest classifers with scikit-learn on this very dataset you have prepared!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works cited & further reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Full readme file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full readme file accompanying data:\n",
    "\n",
    "```\n",
    "Citation Request:\n",
    "   This breast cancer databases was obtained from the University of Wisconsin\n",
    "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
    "   when using this database, then please include this information in your\n",
    "   acknowledgements.  Also, please cite one or more of:\n",
    "\n",
    "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
    "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
    "\n",
    "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
    "      pattern separation for medical diagnosis applied to breast cytology\", \n",
    "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
    "      December 1990, pp 9193-9196.\n",
    "\n",
    "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
    "      via linear programming: Theory and application to medical diagnosis\", \n",
    "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
    "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
    "\n",
    "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
    "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
    "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
    "\n",
    "1. Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
    "\n",
    "2. Sources:\n",
    "   -- Dr. WIlliam H. Wolberg (physician)\n",
    "      University of Wisconsin Hospitals\n",
    "      Madison, Wisconsin\n",
    "      USA\n",
    "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
    "      Received by David W. Aha (aha@cs.jhu.edu)\n",
    "   -- Date: 15 July 1992\n",
    "\n",
    "3. Past Usage:\n",
    "\n",
    "   Attributes 2 through 10 have been used to represent instances.\n",
    "   Each instance has one of 2 possible classes: benign or malignant.\n",
    "\n",
    "   1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of \n",
    "      pattern separation for medical diagnosis applied to breast cytology. In\n",
    "      {\\it Proceedings of the National Academy of Sciences}, {\\it 87},\n",
    "      9193--9196.\n",
    "      -- Size of data set: only 369 instances (at that point in time)\n",
    "      -- Collected classification results: 1 trial only\n",
    "      -- Two pairs of parallel hyperplanes were found to be consistent with\n",
    "         50% of the data\n",
    "         -- Accuracy on remaining 50% of dataset: 93.5%\n",
    "      -- Three pairs of parallel hyperplanes were found to be consistent with\n",
    "         67% of data\n",
    "         -- Accuracy on remaining 33% of dataset: 95.9%\n",
    "\n",
    "   2. Zhang,~J. (1992). Selecting typical instances in instance-based\n",
    "      learning.  In {\\it Proceedings of the Ninth International Machine\n",
    "      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan\n",
    "      Kaufmann.\n",
    "      -- Size of data set: only 369 instances (at that point in time)\n",
    "      -- Applied 4 instance-based learning algorithms \n",
    "      -- Collected classification results averaged over 10 trials\n",
    "      -- Best accuracy result: \n",
    "         -- 1-nearest neighbor: 93.7%\n",
    "         -- trained on 200 instances, tested on the other 169\n",
    "      -- Also of interest:\n",
    "         -- Using only typical instances: 92.2% (storing only 23.1 instances)\n",
    "         -- trained on 200 instances, tested on the other 169\n",
    "\n",
    "4. Relevant Information:\n",
    "\n",
    "   Samples arrive periodically as Dr. Wolberg reports his clinical cases.\n",
    "   The database therefore reflects this chronological grouping of the data.\n",
    "   This grouping information appears immediately below, having been removed\n",
    "   from the data itself:\n",
    "\n",
    "     Group 1: 367 instances (January 1989)\n",
    "     Group 2:  70 instances (October 1989)\n",
    "     Group 3:  31 instances (February 1990)\n",
    "     Group 4:  17 instances (April 1990)\n",
    "     Group 5:  48 instances (August 1990)\n",
    "     Group 6:  49 instances (Updated January 1991)\n",
    "     Group 7:  31 instances (June 1991)\n",
    "     Group 8:  86 instances (November 1991)\n",
    "     -----------------------------------------\n",
    "     Total:   699 points (as of the donated datbase on 15 July 1992)\n",
    "\n",
    "   Note that the results summarized above in Past Usage refer to a dataset\n",
    "   of size 369, while Group 1 has only 367 instances.  This is because it\n",
    "   originally contained 369 instances; 2 were removed.  The following\n",
    "   statements summarizes changes to the original Group 1's set of data:\n",
    "\n",
    "   #####  Group 1 : 367 points: 200B 167M (January 1989)\n",
    "   #####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\n",
    "   #####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\n",
    "   #####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\n",
    "   #####                  : Changed 0 to 1 in field 6 of sample 1219406\n",
    "   #####                  : Changed 0 to 1 in field 8 of following sample:\n",
    "   #####                  : 1182404,2,3,1,1,1,2,0,1,1,1\n",
    "\n",
    "5. Number of Instances: 699 (as of 15 July 1992)\n",
    "\n",
    "6. Number of Attributes: 10 plus the class attribute\n",
    "\n",
    "7. Attribute Information: (class attribute has been moved to last column)\n",
    "\n",
    "   #  Attribute                     Domain\n",
    "   -- -----------------------------------------\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "  10. Mitoses                       1 - 10\n",
    "  11. Class:                        (2 for benign, 4 for malignant)\n",
    "\n",
    "8. Missing attribute values: 16\n",
    "\n",
    "   There are 16 instances in Groups 1 to 6 that contain a single missing \n",
    "   (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
    "\n",
    "9. Class distribution:\n",
    " \n",
    "   Benign: 458 (65.5%)\n",
    "   Malignant: 241 (34.5%)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
